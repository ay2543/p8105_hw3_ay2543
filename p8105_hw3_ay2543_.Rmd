---
title: "p8105_hw3"
output: github_document
---

Before we start anything, load the required libraries:
```{r, message = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)
```

## Problem 1

### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```


## Problem 2

### Loading, tidying and wrangling data

```{r}
accel = read_csv("accel_data.csv")

accel = accel %>% 
  janitor::clean_names() %>% 
    pivot_longer(activity_1:activity_1440,
               names_to = "minute_since_midnight",
               names_prefix = "activity_",
               values_to = "activity") %>% 
  mutate(day_type = ifelse((day == "Saturday" | day == "Sunday"), "weekend", "weekday"),
         day = factor(day, 
                      levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"), 
                      ordered = TRUE),
         minute_since_midnight = as.numeric(minute_since_midnight))


accel %>% 
  head() %>% 
  knitr::kable()

```

The tidied dataset has **`r nrow(accel)` observations and `r ncol(accel)` variables**. 
Key variables are *week, day_id, day, minute_since_midnight to indicate the time of day, activity, and a day_type variable to distinguish weekdays from weekends*.

### Trends of total activity for each day
I created a total activity variable for each day and created a table showing these totals:
```{r}

totals = accel %>% 
  group_by(week, day) %>% 
  summarise(total_activity = sum(activity)) %>% 
  pivot_wider(names_from = day,
              values_from = total_activity)

totals %>% 
  knitr::kable()
```
The mean daily activity across the 5 weeks increases from Monday to Friday, and then drops during the weekend.
The overall mean daily activity was `r mean(totals$daily_total)`.


### Visualization of 24-hour activity time courses 
* Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.
Now let's check for trends in the daily total.

```{r}
accel %>% ggplot(aes(x = minute_since_midnight/60, y = activity)) +
  geom_line(aes(color = day), alpha = 0.5) +
  labs(
    x = "Hour",
    y = "Activity level"
  ) +
  theme_classic() 
```

Observations:

* Low activity level from hour 0 to hour 5
* Activity level is higher from hour 5 to hour 19
* Activity level peaks from around hour 20 to 22 from Monday to Saturday
* Sunday peak activity levels are from around hour 10 to hour 12
* Activity levels drop from hour 22 to 24



## Problem 3

### Load data
```{r}
data("ny_noaa")
```

#### Description of dataset

The ny_noaa dataset has **`r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` observations**. The variables are *`r colnames(ny_noaa)`*. 

Here are the counts and proportions of missing data:

* prcp: `r sum(is.na(ny_noaa$prcp))` (`r sum(is.na(ny_noaa$prcp))/nrow(ny_noaa)`)
* snow: `r sum(is.na(ny_noaa$snow))` (`r sum(is.na(ny_noaa$snow))/nrow(ny_noaa)`)
* snwd: `r sum(is.na(ny_noaa$snwd))` (`r sum(is.na(ny_noaa$snwd))/nrow(ny_noaa)`)
* tmax: `r sum(is.na(ny_noaa$tmax))` (`r sum(is.na(ny_noaa$tmax))/nrow(ny_noaa)`)
* tmin: `r sum(is.na(ny_noaa$tmin))` (`r sum(is.na(ny_noaa$tmin))/nrow(ny_noaa)`)

Almost 50% of entries have a missing tmax and tmin value, and the other variables have varying numbers of missing data. I would be careful in interpreting temperature data.

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

### Data cleaning

```{r}
ny_noaa = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE),
         tmax = as.numeric(tmax),
         tmax = tmax/10,
         tmin = as.numeric(tmin),
         tmin = tmin/10,
         prcp = prcp/10) 
ny_noaa
```

#### Most commonly observed calues for snowfall
```{r}
ny_noaa %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values for snowfall are 0 mm , followed by missing (NA) values. Then the most common non-zero and non-missing snowfall values are 25 mm, 13 mm and 51 mm.


